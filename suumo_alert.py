import requests
from bs4 import BeautifulSoup
import json
import os
import re

# SUUMOæ¤œç´¢URLï¼ˆå·è¥¿èƒ½å‹¢å£é§…ãƒ»2LDKä»¥ä¸Šï¼‰
SEARCH_URL = "https://suumo.jp/chintai/hyogo/ek_10110/nj_207/"
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/117.0.0.0 Safari/537.36"
    )
}
STORED_FILE = "prev_results.json"

def fetch_listings():
    """SUUMOã‹ã‚‰ç‰©ä»¶ãƒªã‚¹ãƒˆã‚’å–å¾—"""
    resp = requests.get(SEARCH_URL, headers=HEADERS)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    items = []
    for el in soup.select(".cassetteitem"):
        title_el = el.select_one(".cassetteitem_content-title a")
        if not title_el:
            continue
        title = title_el.get_text(strip=True)
        link = "https://suumo.jp" + title_el["href"]

        try:
            walk_text = el.select_one(".cassetteitem_detail-text").get_text(strip=True)
            walk_min = int(re.search(r"(\d+)åˆ†", walk_text).group(1))
        except Exception:
            walk_min = 99

        layout = el.select_one(".cassetteitem_madori").get_text(strip=True)
        items.append({
            "title": title,
            "link": link,
            "walk_min": walk_min,
            "layout": layout
        })
    return items

def filter_condition(items):
    """å¾’æ­©10åˆ†ä»¥å†…ãƒ»2LDKä»¥ä¸Šã‚’æŠ½å‡º"""
    result = []
    for it in items:
        if it["walk_min"] <= 10 and (
            it["layout"].startswith("2LDK")
            or it["layout"].startswith("3LDK")
            or it["layout"].startswith("4LDK")
        ):
            result.append(it)
    return result

def load_prev():
    try:
        with open(STORED_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return []

def save_now(items):
    with open(STORED_FILE, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)

def detect_new(prev, now):
    prev_links = {p["link"] for p in prev}
    return [it for it in now if it["link"] not in prev_links]

def notify_slack(items):
    """Slacké€šçŸ¥"""
    webhook = os.getenv("SLACK_WEBHOOK")
    if not webhook:
        print("âŒ SLACK_WEBHOOK not found in environment variables.")
        return

    if not items:
        print("No new listings to notify.")
        return

    text = "*ğŸ  SUUMO æ–°ç€ç‰©ä»¶æƒ…å ±*\n"
    for it in items:
        text += f"â€¢ <{it['link']}|{it['title']}>ï¼ˆå¾’æ­©{it['walk_min']}åˆ† / {it['layout']})\n"

    payload = {"text": text}

    try:
        response = requests.post(webhook, json=payload)
        if response.status_code == 200:
            print(f"âœ… Slacké€šçŸ¥æˆåŠŸ: {len(items)}ä»¶ã®æ–°ç€ç‰©ä»¶ã‚’é€šçŸ¥ã—ã¾ã—ãŸã€‚")
        else:
            print(f"âŒ Slacké€šçŸ¥å¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
            print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {response.text}")
    except Exception as e:
        print(f"âŒ Slacké€šçŸ¥ä¸­ã«ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def main():
    now = fetch_listings()
    filtered = filter_condition(now)
    prev = load_prev()
    new_items = detect_new(prev, filtered)
    if new_items:
        notify_slack(new_items)
    else:
        print("æ–°ç€ç‰©ä»¶ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    save_now(filtered)

if __name__ == "__main__":
    main()
